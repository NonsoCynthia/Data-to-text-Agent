{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/GEM24_EvalLLM/blob/main/GEM24_EvalLLM_OpenAI_SM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "BWaHAvGj8gCb"
      },
      "outputs": [],
      "source": [
        "#@title Install OpenAI\n",
        "from IPython.display import clear_output\n",
        "\n",
        "! pip install openai==0.28\n",
        "# !pip install --upgrade openai\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "fKegUHmi3u52"
      },
      "outputs": [],
      "source": [
        "#@title Download and Load human-eval-packaged json, and format contents (triples, text, id)\n",
        "import json\n",
        "import codecs\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import time\n",
        "\n",
        "language = \"English\"\n",
        "\n",
        "def format_json(json_path):\n",
        "  # Open en_regular and parse json\n",
        "  en_regular_json = json.load(codecs.open(json_path, 'r', 'utf-8'))\n",
        "  # Print first entry\n",
        "  # print(json.dumps(en_regular_json[0], indent=4))\n",
        "\n",
        "  triples_text_pairs = []\n",
        "\n",
        "  x = 0\n",
        "  while x < len(en_regular_json):\n",
        "    # if x < 10:\n",
        "    # Parse html found in the \"input\" key\n",
        "    html = en_regular_json[x]['input']\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    # Print raw table\n",
        "    # print(soup.prettify())\n",
        "    table = soup.find('table')\n",
        "    # headers = [header.text.strip() for header in table.find_all('th')]\n",
        "    rows = []\n",
        "    for row in table.find_all('tr'):\n",
        "      columns = row.find_all(['td', 'th'])  # Get both <td> and <th>\n",
        "      row_data = ' '.join([col.text.strip() for col in columns])\n",
        "      rows.append(row_data)\n",
        "    triples_formatted = '; '.join(rows[1:]) # exclude header\n",
        "    # print(\"Headers:\", rows[0])\n",
        "    # print(rows[1:])\n",
        "    triples_text_pairs.append({'id':en_regular_json[x]['id'], 'triples': '\"\"\"'+triples_formatted+'\"\"\"', 'text': en_regular_json[x]['output']})\n",
        "    # else:\n",
        "    #   break\n",
        "    x += 1\n",
        "  return triples_text_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "720 datapoints found!\n",
            "{'id': 'en_D2T-1-FA_1437_3_struct_D2T', 'triples': '\"\"\"McVeagh_of_the_South_Seas director Cyril_Bruce; McVeagh_of_the_South_Seas director Harry_Carey_(actor_born_1878); McVeagh_of_the_South_Seas writer Harry_Carey_(actor_born_1878); McVeagh_of_the_South_Seas producer The_Progressive_Motion_Picture_Company; McVeagh_of_the_South_Seas distributor Alliance_Films_Corporation\"\"\"', 'text': 'The film McVeagh of the South Seas was directed by Cyril Bruce and Harry Carey, and distributed by Alliance Films Corporation.'}\n"
          ]
        }
      ],
      "source": [
        "#@title Load Custom json file\n",
        "import json\n",
        "import codecs\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "language = \"English\" #@param[\"English\", \"Spanish\", \"Swahili\"]\n",
        "\n",
        "custom_filepath = 'llm_as_judge_samples.json'\n",
        "triples_text_pairs = format_json(custom_filepath)\n",
        "print(f'{len(triples_text_pairs)} datapoints found!')\n",
        "print(triples_text_pairs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get already processed IDs\n",
        "path_out = 'GPT_results'\n",
        "existing_files = os.listdir(path_out)\n",
        "processed_ids = set()\n",
        "for fname in existing_files:\n",
        "    if fname.startswith('GPT_results_'):\n",
        "        processed_ids.add(fname[len('GPT_results_'):])\n",
        "\n",
        "\n",
        "# existing_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "ofwSLwpTyK9U"
      },
      "outputs": [],
      "source": [
        "#@title Run evaluation (needs OpenAI API key in Parameters)\n",
        "\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "import openai\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "# PARAMETERS OpenAI\n",
        "#==========================\n",
        "# openai.api_key = \"insertYourKeyHere\"\n",
        "model=\"o3\" #@param[\"gpt-4o-mini-2024-07-18\", \"o3-mini-2025-01-31\"]\n",
        "path_out = 'GPT_results'\n",
        "#==========================\n",
        "\n",
        "if not os.path.exists(path_out):\n",
        "  os.makedirs(path_out)\n",
        "\n",
        "def dumpResults(annotations, path_out):\n",
        "  results_file = open(os.path.join(path_out, 'All_GPT_results'), 'ab')\n",
        "  pickle.dump(annotations, results_file)\n",
        "  results_file.close()\n",
        "\n",
        "def callGPT(prompt, Triples, Nice_Text, model):\n",
        "    response =  openai.ChatCompletion.create(\n",
        "    model=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\",\n",
        "         \"content\": prompt},\n",
        "    ],\n",
        "    temperature=1)\n",
        "\n",
        "    return response\n",
        "\n",
        "def runEval(triples_text_pairs, model):\n",
        "  # EN regular splits: range(0, 2750), range(2750, 5500), range(5500, 8240)\n",
        "  x = 0\n",
        "  # To get all evaluations\n",
        "  for x in range(0, len(triples_text_pairs)):\n",
        "  # To test on a few inputs only\n",
        "  # while x < 3:\n",
        "    Triples = triples_text_pairs[x]['triples']\n",
        "    Nice_Text = triples_text_pairs[x]['text']\n",
        "    id = triples_text_pairs[x]['id']\n",
        "    if id in processed_ids:\n",
        "      # print(f\"Skipping text #{x} (ID={id}) -- already processed.\")\n",
        "      continue\n",
        "\n",
        "    #Prompt (Do not change unless discussed with the GEM-HumEval group)\n",
        "    prompt = '''\n",
        "In this task, you will evaluate the quality of the Text in relation to the given Triple Set. How well does the Text represent the Triple Set?  You will be given four specific Dimensions to evaluate against:\n",
        "\n",
        "Dimensions:\"\"\"\n",
        "No-Omissions: ALL the information in the Triple Set is present in the Text.\n",
        "No-Additions: ONLY information from the Triple Set is present in the Text.\n",
        "Grammaticality: The Text is free of grammatical and spelling errors.\n",
        "Fluency: The Text flows well and is easy to read; its parts are connected in a natural way.\"\"\"\n",
        "\n",
        "Important note on No-Omissions and No-Additions: some Triple Set/Text pairs contain non-factual information and even fictional names for people, places, dates, etc. Whether there are omissions and/or additions in a Text is NOT related to factual truth, but instead is strictly related to the contents of the input Triple Set.\n",
        "Important note on Grammaticality and Fluency: for Grammaticality and Fluency you do not need to consider the input Triple Set; only the intrinsic quality of the Text needs to be assessed.\n",
        "\n",
        "You need to provide the scores ranging from 1 (indicating the lowest score) to 7 (indicating the highest score) for each of the dimensions and a short justification for each score in the following JSON format:  {\"No-Omissions\": {\"Justification\": \"\", \"Score\": \"\"}, \"No-Additions\": {\"Justification\": \"\", \"Score\": \"\"}, \"Grammaticality\": {\"Justification\": \"\", \"Score\": \"\"}, \"Fluency\": {\"Justification\": \"\", \"Score\": \"\"} }.\n",
        "\n",
        "Make sure to read thoroughly the Triple Set and the '''+str(language)+''' Text below, and assess the four Dimensions using the instructions and template above.\n",
        "\n",
        "Triple Set: ''' + str(Triples) + \"\\n\" + '''Text: '''+ str(Nice_Text) + \"\\n\\n\" + '''\n",
        "'''\n",
        "    print(f'Evaluating text #{x}...')\n",
        "    print(f'ID: {id}')\n",
        "    # print(prompt)\n",
        "    # print(Triples)\n",
        "    # print(Nice_Text)\n",
        "    responseGPT = callGPT(prompt, Triples, Nice_Text, model)\n",
        "    # print(responseGPT.choices[0].message.content)\n",
        "    # print('\\n')\n",
        "    # print(response['choices'][0]['message']['content'])\n",
        "    # print('\\n')\n",
        "    triples_text_pairs[x]['scores_GPT'] = responseGPT['choices'][0]['message']['content']\n",
        "\n",
        "    # Save individual files as backup\n",
        "    # with open(os.path.join('GPT_results', 'GPT_results_'+str(id)), 'ab') as f:\n",
        "    with open(os.path.join('GPT_results', 'GPT_results_' + id), 'ab') as f:\n",
        "      pickle.dump(triples_text_pairs[x], f)\n",
        "\n",
        "    time.sleep(10)\n",
        "    x += 1\n",
        "\n",
        "  return triples_text_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "annotations = runEval(triples_text_pairs, model)\n",
        "\n",
        "# dumpResults(annotations, path_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twtHvjIruiSm"
      },
      "source": [
        "## Results analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "cQDPBbTqIVOW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "en_D2T-1-FA_1237_1_agent_D2T\n",
            "en_D2T-1-FA_0559_4_human_D2T\n",
            "en_D2T-1-FA_0701_2_e2e_D2T\n",
            "en_D2T-1-FA_1610_2_e2e_D2T\n",
            "en_D2T-1-FA_0083_4_human_D2T\n",
            "en_D2T-1-FA_1430_4_human_D2T\n",
            "en_D2T-1-FA_1655_2_e2e_D2T\n",
            "en_D2T-1-FA_1638_4_human_D2T\n",
            "en_D2T-1-FA_1655_3_struct_D2T\n",
            "en_D2T-1-FA_0745_1_agent_D2T\n",
            "en_D2T-1-FA_0667_3_struct_D2T\n",
            "en_D2T-1-FA_1588_2_e2e_D2T\n",
            "en_D2T-1-FA_1661_2_e2e_D2T\n",
            "en_D2T-1-FA_0604_3_struct_D2T\n",
            "en_D2T-1-FA_1437_3_struct_D2T\n",
            "en_D2T-1-FA_0360_1_agent_D2T\n",
            "en_D2T-1-FA_0585_3_struct_D2T\n",
            "en_D2T-1-FA_0212_3_struct_D2T\n",
            "en_D2T-1-FA_1722_4_human_D2T\n",
            "en_D2T-1-FA_0585_1_agent_D2T\n",
            "en_D2T-1-FA_1454_4_human_D2T\n",
            "en_D2T-1-FA_0935_4_human_D2T\n",
            "en_D2T-1-FA_1025_4_human_D2T\n",
            "\n",
            "=== JSON LOAD FAIL ===\n",
            "Offending string:\n",
            " {\n",
            "\"No-Omissions\": {\n",
            "\"Justification\": \"\", \"Score\": \"7\"\n",
            "},\n",
            "\"No-Additions\": {\n",
            "\"Justification\": \"\", \"Score\": \"7\"\n",
            "},\n",
            "\"Grammaticality\": {\n",
            "\"Justification\": \"\", \"Score\": \"7\"\n",
            "},\n",
            "\"Fluency\": {\n",
            "\"Justification\": \"\", \"Score\": \"6\"\n",
            "}\n",
            "=====================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:33: SyntaxWarning: invalid escape sequence '\\{'\n",
            "<>:35: SyntaxWarning: invalid escape sequence '\\g'\n",
            "<>:36: SyntaxWarning: invalid escape sequence '\\g'\n",
            "<>:33: SyntaxWarning: invalid escape sequence '\\{'\n",
            "<>:35: SyntaxWarning: invalid escape sequence '\\g'\n",
            "<>:36: SyntaxWarning: invalid escape sequence '\\g'\n",
            "/tmp/ipykernel_591986/1100273735.py:33: SyntaxWarning: invalid escape sequence '\\{'\n",
            "  search_expression = '(\"'+criterion+'\":[^\\{]+\\{[^\\}]*\"Justification\":)([^\\}]+)(\"Score\":[^\\}]+\\})'\n",
            "/tmp/ipykernel_591986/1100273735.py:35: SyntaxWarning: invalid escape sequence '\\g'\n",
            "  justificationRemoved = re.sub(search_expression, '\\g<1> \"\", \\g<3>',  LLMoutString)\n",
            "/tmp/ipykernel_591986/1100273735.py:36: SyntaxWarning: invalid escape sequence '\\g'\n",
            "  justification = re.sub('^.*'+search_expression+'.*$', '\\g<2>',  LLMoutString)\n"
          ]
        },
        {
          "ename": "SyntaxError",
          "evalue": "'{' was never closed (<unknown>, line 1)",
          "output_type": "error",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "\u001b[0m  Cell \u001b[1;32mIn[21], line 92\u001b[0m in \u001b[1;35mloadDataPoint\u001b[0m\n    scores_json = json.loads(LLMout_string)\u001b[0m\n",
            "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.12/json/__init__.py:346\u001b[0m in \u001b[1;35mloads\u001b[0m\n    return _default_decoder.decode(s)\u001b[0m\n",
            "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.12/json/decoder.py:337\u001b[0m in \u001b[1;35mdecode\u001b[0m\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m~/anaconda3/lib/python3.12/json/decoder.py:353\u001b[0;36m in \u001b[0;35mraw_decode\u001b[0;36m\n\u001b[0;31m    obj, end = self.scan_once(s, idx)\u001b[0;36m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m\u001b[0;31m:\u001b[0m Expecting ',' delimiter\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
            "\u001b[0m  Cell \u001b[1;32mIn[21], line 137\u001b[0m\n    formatted_scores, eval_missing, wrong_score = loadDataPoint(dbfile_x, model_prefix)\u001b[0m\n",
            "\u001b[0m  Cell \u001b[1;32mIn[21], line 100\u001b[0m in \u001b[1;35mloadDataPoint\u001b[0m\n    raise e\u001b[0m\n",
            "\u001b[0m  Cell \u001b[1;32mIn[21], line 95\u001b[0m in \u001b[1;35mloadDataPoint\u001b[0m\n    scores_json = ast.literal_eval(LLMout_string)\u001b[0m\n",
            "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.12/ast.py:66\u001b[0m in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m~/anaconda3/lib/python3.12/ast.py:52\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0;36m\n",
            "\u001b[0;36m  File \u001b[0;32m<unknown>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    {\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '{' was never closed\n"
          ]
        }
      ],
      "source": [
        "#@title Load unzipped files\n",
        "import pickle\n",
        "import glob\n",
        "import json\n",
        "import re\n",
        "import codecs\n",
        "import os\n",
        "import ast\n",
        "\n",
        "def auto_close_json(s):\n",
        "    # Counts how many more } are needed\n",
        "    open_braces = s.count('{')\n",
        "    close_braces = s.count('}')\n",
        "    needed = open_braces - close_braces\n",
        "    if needed > 0:\n",
        "        s += '}' * needed\n",
        "    return s\n",
        "\n",
        "update_params_unzip = True #@param {type:\"boolean\"}\n",
        "if update_params_unzip:\n",
        "  zip_language = 'EN' #@param['EN', 'ES', 'SW']\n",
        "  zip_model = 'o3' #@param['GPT-4o-mini', 'GPT-o3-mini', 'Gemini-1dot5-flash']\n",
        "  zip_data = 'regular' #@param['regular', 'iaa']\n",
        "\n",
        "model_scores = [1, 2, 3, 4, 5, 6, 7]\n",
        "# load_gemini_folder = True #@param {type:\"boolean\"}\n",
        "# load_gpt_folder = False #@param {type:\"boolean\"}\n",
        "path_dir_unzipped = ''\n",
        "model_prefix = ''\n",
        "if zip_model.startswith('Gemini'):\n",
        "  path_dir_unzipped = os.path.join('/content', zip_language+'_'+zip_data, zip_model, 'content', 'Gemini_results')\n",
        "  model_prefix = 'Gemini'\n",
        "# elif zip_model.startswith('GPT'):\n",
        "else:\n",
        "  path_dir_unzipped = os.path.join('GPT_results')\n",
        "  model_prefix = 'GPT'\n",
        "\n",
        "def separateJustification(LLMoutString, criterion):\n",
        "  \"\"\"\n",
        "  The Justifications returned by the models often break the json format, so I extract them\n",
        "  \"\"\"\n",
        "  search_expression = '(\"'+criterion+'\":[^\\{]+\\{[^\\}]*\"Justification\":)([^\\}]+)(\"Score\":[^\\}]+\\})'\n",
        "  if re.search(search_expression, LLMoutString):\n",
        "    justificationRemoved = re.sub(search_expression, '\\g<1> \"\", \\g<3>',  LLMoutString)\n",
        "    justification = re.sub('^.*'+search_expression+'.*$', '\\g<2>',  LLMoutString)\n",
        "  else:\n",
        "    justificationRemoved = LLMoutString\n",
        "    justification = ''\n",
        "  return justificationRemoved, justification\n",
        "\n",
        "def loadDataPoint(dbfile_x, model):\n",
        "  eval_missing = None\n",
        "  wrong_score = None\n",
        "  dico_key = 'scores_'+str(model)\n",
        "  formatted_scores = {}\n",
        "  # load data with pickle\n",
        "  dp = pickle.load(dbfile_x)\n",
        "  if dico_key in dp:\n",
        "    print(dp['id'])\n",
        "    # print(dp['triples'])\n",
        "    # print(dp['text'])\n",
        "    justifications = []\n",
        "    # pickle.load uses single quotes, whereas json.load expects double quotes\n",
        "    # Gemini adds a node \"query\" in the json, unlike OpenAI's models\n",
        "    LLMout_string = str(dp[dico_key]).replace(\"'query'\", '\"query\"')\n",
        "    LLMout_string = LLMout_string.replace(\"```json\", \"\")\n",
        "    LLMout_string = LLMout_string.replace(\"```\", \"\")\n",
        "    LLMout_string = LLMout_string.replace(\"'No-Omissions'\", '\"No-Omissions\"')\n",
        "    # There's a typo in one of the Gemini outputs\n",
        "    LLMout_string = LLMout_string.replace(\"'No-Omissons'\", '\"No-Omissions\"')\n",
        "    LLMout_string = LLMout_string.replace('\"No-Omissons\"', '\"No-Omissions\"')\n",
        "    LLMout_string = LLMout_string.replace(\"'No-Additions'\", '\"No-Additions\"')\n",
        "    LLMout_string = LLMout_string.replace(\"'Grammaticality'\", '\"Grammaticality\"')\n",
        "    LLMout_string = LLMout_string.replace(\"'Fluency'\", '\"Fluency\"')\n",
        "    # Sometimes justifications are followed by single quotes, sometimes by double quotes\n",
        "    LLMout_string = LLMout_string.replace(\"'Justification': '\", '\"Justification\": \"').replace(\"'Justification'\", '\"Justification\"')\n",
        "    LLMout_string = LLMout_string.replace(\"', 'Score'\", '\", \"Score\"').replace(\"'Score'\", '\"Score\"')\n",
        "    if LLMout_string == 'None':\n",
        "      eval_missing = dp['id']\n",
        "    else:\n",
        "      # print(LLMout_string)\n",
        "      # LLMout_string = re.sub('(\"No-Omissions\":[^\\{]+\\{\"Justification\":)([^\\}]+)(\"Score\":[^\\}]+\\})', '\\g<1> \"\", \\g<3>',  LLMout_string)\n",
        "      LLMout_string, justifNoOm = separateJustification(LLMout_string, 'No-Omissions')\n",
        "      justifications.append(justifNoOm)\n",
        "      LLMout_string, justifNoAdd = separateJustification(LLMout_string, 'No-Additions')\n",
        "      justifications.append(justifNoAdd)\n",
        "      LLMout_string, justifGram = separateJustification(LLMout_string, 'Grammaticality')\n",
        "      justifications.append(justifGram)\n",
        "      LLMout_string, justifFlu = separateJustification(LLMout_string, 'Fluency')\n",
        "      justifications.append(justifFlu)\n",
        "      # print(justifications)\n",
        "      LLMout_string = LLMout_string.replace(\"'1'\", '\"1\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'2'\", '\"2\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'3'\", '\"3\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'4'\", '\"4\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'5'\", '\"5\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'6'\", '\"6\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'7'\", '\"7\"')\n",
        "      LLMout_string = auto_close_json(LLMout_string)\n",
        "      # scores_json = json.loads(LLMout_string)\n",
        "      try:\n",
        "        scores_json = json.loads(LLMout_string)\n",
        "      except Exception:\n",
        "          try:\n",
        "              scores_json = ast.literal_eval(LLMout_string)\n",
        "          except Exception as e:\n",
        "              print(\"\\n=== JSON LOAD FAIL ===\")\n",
        "              print(\"Offending string:\\n\", LLMout_string)\n",
        "              print(\"=====================\\n\")\n",
        "              raise e\n",
        "      clean_scores_json = None\n",
        "      # Gemini adds a node \"query\" in the json, unlike OpenAI's models\n",
        "      if 'query' in scores_json:\n",
        "        clean_scores_json = scores_json['query']\n",
        "      else:\n",
        "        clean_scores_json = scores_json\n",
        "\n",
        "      gram_score = int(clean_scores_json['Grammaticality']['Score'])\n",
        "      flu_score = int(clean_scores_json['Fluency']['Score'])\n",
        "      no_om_score = int(clean_scores_json['No-Omissions']['Score'])\n",
        "      no_ad_score = int(clean_scores_json['No-Additions']['Score'])\n",
        "\n",
        "      if (gram_score not in model_scores) or (flu_score not in model_scores) or (no_om_score not in model_scores) or (no_ad_score not in model_scores):\n",
        "        wrong_score = dp['id']\n",
        "\n",
        "      formatted_scores[\"eid\"] = dp['id']\n",
        "      formatted_scores[\"annotator_id\"] = str(zip_model)\n",
        "      formatted_scores[\"no-omissions\"] = no_om_score\n",
        "      formatted_scores[\"no-additions\"] = no_ad_score\n",
        "      formatted_scores[\"grammaticality\"] = gram_score\n",
        "      formatted_scores[\"fluency\"] = flu_score\n",
        "\n",
        "      # print(f\"Gram: {gram_score}; Flu: {flu_score}; NoOm: {no_om_score}; NoAd: {no_ad_score}.\")\n",
        "      # print('')\n",
        "\n",
        "  return formatted_scores, eval_missing, wrong_score\n",
        "\n",
        "# print(path_dir_unzipped)\n",
        "# print(model_prefix)\n",
        "eval_files = glob.glob(os.path.join(path_dir_unzipped, '*'))\n",
        "evals_missing = []\n",
        "wrong_scores = []\n",
        "all_scores = []\n",
        "for filepath in eval_files:\n",
        "  # print(filepath)\n",
        "  dbfile_x = open(filepath, 'rb')\n",
        "  formatted_scores, eval_missing, wrong_score = loadDataPoint(dbfile_x, model_prefix)\n",
        "  if eval_missing != None:\n",
        "    evals_missing.append(eval_missing)\n",
        "  if wrong_score != None:\n",
        "    wrong_scores.append(wrong_score)\n",
        "  dbfile_x.close()\n",
        "  all_scores.append(formatted_scores)\n",
        "# print(f'Missing evaluations: {evals_missing}')\n",
        "# print(f'Wrong scores: {wrong_scores}')\n",
        "\n",
        "# Save all scores into a json file\n",
        "path_json_out = zip_language+'_'+zip_model+'_scores.json'\n",
        "with codecs.open(path_json_out, 'w', 'utf-8') as outfile:\n",
        "  json.dump(all_scores, outfile)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
