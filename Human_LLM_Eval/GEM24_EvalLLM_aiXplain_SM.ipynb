{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/GEM24_EvalLLM/blob/main/GEM24_EvalLLM_aiXplain_SM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "BWaHAvGj8gCb"
      },
      "outputs": [],
      "source": [
        "#@title Install Aixplain\n",
        "from IPython.display import clear_output\n",
        "! pip install aixplain\n",
        "clear_output()\n",
        "\n",
        "def format_json(json_path):\n",
        "  # Open en_regular and parse json\n",
        "  en_regular_json = json.load(codecs.open(json_path, 'r', 'utf-8'))\n",
        "  # Print first entry\n",
        "  # print(json.dumps(en_regular_json[0], indent=4))\n",
        "\n",
        "  triples_text_pairs = []\n",
        "\n",
        "  x = 0\n",
        "  while x < len(en_regular_json):\n",
        "    # if x < 10:\n",
        "    # Parse html found in the \"input\" key\n",
        "    html = en_regular_json[x]['input']\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    # Print raw table\n",
        "    # print(soup.prettify())\n",
        "    table = soup.find('table')\n",
        "    # headers = [header.text.strip() for header in table.find_all('th')]\n",
        "    rows = []\n",
        "    for row in table.find_all('tr'):\n",
        "      columns = row.find_all(['td', 'th'])  # Get both <td> and <th>\n",
        "      row_data = ' '.join([col.text.strip() for col in columns])\n",
        "      rows.append(row_data)\n",
        "    triples_formatted = '; '.join(rows[1:]) # exclude header\n",
        "    # print(\"Headers:\", rows[0])\n",
        "    # print(rows[1:])\n",
        "    triples_text_pairs.append({'id':en_regular_json[x]['id'], 'triples': '\"\"\"'+triples_formatted+'\"\"\"', 'text': en_regular_json[x]['output']})\n",
        "    # else:\n",
        "    #   break\n",
        "    x += 1\n",
        "  return triples_text_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cellView": "form",
        "id": "VtqmdrBLCwwl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "720 datapoints found!\n",
            "{'id': 'en_D2T-1-FA_1437_3_struct_D2T', 'triples': '\"\"\"McVeagh_of_the_South_Seas director Cyril_Bruce; McVeagh_of_the_South_Seas director Harry_Carey_(actor_born_1878); McVeagh_of_the_South_Seas writer Harry_Carey_(actor_born_1878); McVeagh_of_the_South_Seas producer The_Progressive_Motion_Picture_Company; McVeagh_of_the_South_Seas distributor Alliance_Films_Corporation\"\"\"', 'text': 'The film McVeagh of the South Seas was directed by Cyril Bruce and Harry Carey, and distributed by Alliance Films Corporation.'}\n"
          ]
        }
      ],
      "source": [
        "#@title Load Custom json file\n",
        "import json\n",
        "import codecs\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "language = \"English\" #@param[\"English\", \"Spanish\", \"Swahili\"]\n",
        "\n",
        "custom_filepath = 'llm_as_judge_samples.json'\n",
        "triples_text_pairs = format_json(custom_filepath)\n",
        "print(f'{len(triples_text_pairs)} datapoints found!')\n",
        "print(triples_text_pairs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Run evaluation (needs aiXplain API key in Parameters)\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import pickle\n",
        "import time\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())  # Load environment variables\n",
        "\n",
        "# PARAMETERS aiXplain\n",
        "#==========================\n",
        "os.environ[\"TEAM_API_KEY\"] = os.getenv(\"TEAM_API_KEY\")\n",
        "Gemini_1dot5_Flash = '67e301b243d4fa5705dfa725' # Gemini 2.5 Pro Experimental\n",
        "path_out = 'Gemini_results'\n",
        "#==========================\n",
        "from aixplain.factories import AgentFactory\n",
        "# from aixplain.modules.agent import ModelTool\n",
        "from aixplain.modules.agent.tool.model_tool import ModelTool\n",
        "\n",
        "if not os.path.exists(path_out):\n",
        "  os.makedirs(path_out)\n",
        "\n",
        "def dumpResults(annotations, path_out):\n",
        "  results_file = open(os.path.join(path_out, 'All_Gemini_results'), 'ab')\n",
        "  pickle.dump(annotations, results_file)\n",
        "  results_file.close()\n",
        "\n",
        "def callGemini_aiXplain(prompt, model):\n",
        "  agent = AgentFactory.create(\n",
        "    name=\"Assessment of text quality\",\n",
        "\t  description=\"Assessment of text quality\",\n",
        "\t  instructions=\"\",\n",
        "    tools=[\n",
        "      ModelTool(model=model),\n",
        "    ],\n",
        "  )\n",
        "  agent_response = agent.run(prompt)\n",
        "\n",
        "  return agent_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "131"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get already processed IDs\n",
        "# path_out = 'Gemini_results'\n",
        "existing_files = os.listdir(path_out)\n",
        "processed_ids = set()\n",
        "for fname in existing_files:\n",
        "    if fname.startswith('Gemini_results_'):\n",
        "        processed_ids.add(fname[len('Gemini_results_'):])\n",
        "\n",
        "len(existing_files)\n",
        "# for i in processed_ids:\n",
        "#     print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "ofwSLwpTyK9U"
      },
      "outputs": [],
      "source": [
        "def runEval(triples_text_pairs, model):\n",
        "  # EN regular splits: range(0, 2750), range(2750, 5500), range(5500, 8280)\n",
        "  x = 0\n",
        "  # To get all evaluations\n",
        "  for x in range(0, len(triples_text_pairs)):\n",
        "  # for x in range(7413, 8280):\n",
        "  # while x < 3:\n",
        "    Triples = triples_text_pairs[x]['triples']\n",
        "    Nice_Text = triples_text_pairs[x]['text']\n",
        "    id = triples_text_pairs[x]['id']\n",
        "    if id in processed_ids:\n",
        "      # print(f\"Skipping text #{x} (ID={id}) -- already processed.\")\n",
        "      continue\n",
        "\n",
        "    #Prompt (Do not change unless discussed with the GEM-HumEval group)\n",
        "    prompt = '''\n",
        "In this task, you will evaluate the quality of the Text in relation to the given Triple Set. How well does the Text represent the Triple Set?  You will be given four specific Dimensions to evaluate against:\n",
        "\n",
        "Dimensions:\"\"\"\n",
        "No-Omissions: ALL the information in the Triple Set is present in the Text.\n",
        "No-Additions: ONLY information from the Triple Set is present in the Text.\n",
        "Grammaticality: The Text is free of grammatical and spelling errors.\n",
        "Fluency: The Text flows well and is easy to read; its parts are connected in a natural way.\"\"\"\n",
        "\n",
        "Important note on No-Omissions and No-Additions: some Triple Set/Text pairs contain non-factual information and even fictional names for people, places, dates, etc. Whether there are omissions and/or additions in a Text is NOT related to factual truth, but instead is strictly related to the contents of the input Triple Set.\n",
        "Important note on Grammaticality and Fluency: for Grammaticality and Fluency you do not need to consider the input Triple Set; only the intrinsic quality of the Text needs to be assessed.\n",
        "\n",
        "You need to provide the scores ranging from 1 (indicating the lowest score) to 7 (indicating the highest score) for each of the dimensions and a short justification for each score in the following JSON format:  {\"No-Omissions\": {\"Justification\": \"\", \"Score\": \"\"}, \"No-Additions\": {\"Justification\": \"\", \"Score\": \"\"}, \"Grammaticality\": {\"Justification\": \"\", \"Score\": \"\"}, \"Fluency\": {\"Justification\": \"\", \"Score\": \"\"} }.\n",
        "\n",
        "Make sure to read thoroughly the Triple Set and the '''+str(language)+''' Text below, and assess the four Dimensions using the instructions and template above.\n",
        "\n",
        "Triple Set: ''' + str(Triples) + \"\\n\" + '''Text: '''+ str(Nice_Text) + \"\\n\\n\" + '''\n",
        "'''\n",
        "    print(f'Evaluating text #{x}...')\n",
        "    print(f'ID: {id}')\n",
        "    # To run missing evals\n",
        "    # if id == 'en_D2T-1-FA_1318_6' or id == 'en_D2T-1-FA_0342_6':\n",
        "    # if id == 'es_D2T-1-FA_0614_2':\n",
        "    # Begin indent block\n",
        "    responseGemini = callGemini_aiXplain(prompt, model)\n",
        "    # Gemini does not always output correct json format, so I save the raw result at this point\n",
        "    triples_text_pairs[x]['scores_Gemini'] = str(responseGemini['data']['output'])\n",
        "\n",
        "    # Save individual files as backup\n",
        "    with open(os.path.join('Gemini_results', 'Gemini_results_'+str(id)), 'ab') as f:\n",
        "      pickle.dump(triples_text_pairs[x], f)\n",
        "    # End indent block\n",
        "\n",
        "    time.sleep(10)\n",
        "\n",
        "    x += 1\n",
        "\n",
        "  return triples_text_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "annotations = runEval(triples_text_pairs, Gemini_1dot5_Flash)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twtHvjIruiSm"
      },
      "source": [
        "## Results analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "cQDPBbTqIVOW"
      },
      "outputs": [],
      "source": [
        "#@title Load unzipped files\n",
        "import pickle\n",
        "import glob\n",
        "import json\n",
        "import re\n",
        "import codecs\n",
        "\n",
        "update_params_unzip = True #@param {type:\"boolean\"}\n",
        "if update_params_unzip:\n",
        "  zip_language = 'EN' #@param['EN', 'ES', 'SW']\n",
        "  zip_model = 'Gemini' #@param['GPT-4o-mini', 'GPT-o3-mini', 'Gemini-1dot5-flash']\n",
        "  zip_data = 'regular' #@param['regular', 'iaa']\n",
        "\n",
        "model_scores = [1, 2, 3, 4, 5, 6, 7]\n",
        "# load_gemini_folder = True #@param {type:\"boolean\"}\n",
        "# load_gpt_folder = False #@param {type:\"boolean\"}\n",
        "path_dir_unzipped = ''\n",
        "model_prefix = ''\n",
        "if zip_model.startswith('Gemini'):\n",
        "  path_dir_unzipped = os.path.join('Gemini_results')\n",
        "  model_prefix = 'Gemini'\n",
        "elif zip_model.startswith('GPT'):\n",
        "  path_dir_unzipped = os.path.join('GPT_results')\n",
        "  model_prefix = 'GPT'\n",
        "\n",
        "def separateJustification(LLMoutString, criterion):\n",
        "  \"\"\"\n",
        "  The Justifications returned by the models often break the json format, so I extract them\n",
        "  \"\"\"\n",
        "  search_expression = '(\"'+criterion+'\":[^\\{]+\\{[^\\}]*\"Justification\":)([^\\}]+)(\"Score\":[^\\}]+\\})'\n",
        "  if re.search(search_expression, LLMoutString):\n",
        "    justificationRemoved = re.sub(search_expression, '\\g<1> \"\", \\g<3>',  LLMoutString)\n",
        "    justification = re.sub('^.*'+search_expression+'.*$', '\\g<2>',  LLMoutString)\n",
        "  else:\n",
        "    justificationRemoved = LLMoutString\n",
        "    justification = ''\n",
        "  return justificationRemoved, justification\n",
        "\n",
        "def loadDataPoint(dbfile_x, model):\n",
        "  eval_missing = None\n",
        "  wrong_score = None\n",
        "  dico_key = 'scores_'+str(model)\n",
        "  formatted_scores = {}\n",
        "  # load data with pickle\n",
        "  dp = pickle.load(dbfile_x)\n",
        "  if dico_key in dp:\n",
        "    print(dp['id'])\n",
        "    print(dp['triples'])\n",
        "    print(dp['text'])\n",
        "    justifications = []\n",
        "    # pickle.load uses single quotes, whereas json.load expects double quotes\n",
        "    # Gemini adds a node \"query\" in the json, unlike OpenAI's models\n",
        "    LLMout_string = str(dp[dico_key]).replace(\"'query'\", '\"query\"')\n",
        "    LLMout_string = LLMout_string.replace(\"```json\", \"\")\n",
        "    LLMout_string = LLMout_string.replace(\"```\", \"\")\n",
        "    LLMout_string = LLMout_string.replace(\"'No-Omissions'\", '\"No-Omissions\"')\n",
        "    # There's a typo in one of the Gemini outputs\n",
        "    LLMout_string = LLMout_string.replace(\"'No-Omissons'\", '\"No-Omissions\"')\n",
        "    LLMout_string = LLMout_string.replace('\"No-Omissons\"', '\"No-Omissions\"')\n",
        "    LLMout_string = LLMout_string.replace(\"'No-Additions'\", '\"No-Additions\"')\n",
        "    LLMout_string = LLMout_string.replace(\"'Grammaticality'\", '\"Grammaticality\"')\n",
        "    LLMout_string = LLMout_string.replace(\"'Fluency'\", '\"Fluency\"')\n",
        "    # Sometimes justifications are followed by single quotes, sometimes by double quotes\n",
        "    LLMout_string = LLMout_string.replace(\"'Justification': '\", '\"Justification\": \"').replace(\"'Justification'\", '\"Justification\"')\n",
        "    LLMout_string = LLMout_string.replace(\"', 'Score'\", '\", \"Score\"').replace(\"'Score'\", '\"Score\"')\n",
        "    if LLMout_string == 'None':\n",
        "      eval_missing = dp['id']\n",
        "    else:\n",
        "      # print(LLMout_string)\n",
        "      # LLMout_string = re.sub('(\"No-Omissions\":[^\\{]+\\{\"Justification\":)([^\\}]+)(\"Score\":[^\\}]+\\})', '\\g<1> \"\", \\g<3>',  LLMout_string)\n",
        "      LLMout_string, justifNoOm = separateJustification(LLMout_string, 'No-Omissions')\n",
        "      justifications.append(justifNoOm)\n",
        "      LLMout_string, justifNoAdd = separateJustification(LLMout_string, 'No-Additions')\n",
        "      justifications.append(justifNoAdd)\n",
        "      LLMout_string, justifGram = separateJustification(LLMout_string, 'Grammaticality')\n",
        "      justifications.append(justifGram)\n",
        "      LLMout_string, justifFlu = separateJustification(LLMout_string, 'Fluency')\n",
        "      justifications.append(justifFlu)\n",
        "      print(justifications)\n",
        "      LLMout_string = LLMout_string.replace(\"'1'\", '\"1\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'2'\", '\"2\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'3'\", '\"3\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'4'\", '\"4\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'5'\", '\"5\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'6'\", '\"6\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'7'\", '\"7\"')\n",
        "      scores_json = json.loads(LLMout_string)\n",
        "      clean_scores_json = None\n",
        "      # Gemini adds a node \"query\" in the json, unlike OpenAI's models\n",
        "      if 'query' in scores_json:\n",
        "        clean_scores_json = scores_json['query']\n",
        "      else:\n",
        "        clean_scores_json = scores_json\n",
        "\n",
        "      gram_score = int(clean_scores_json['Grammaticality']['Score'])\n",
        "      flu_score = int(clean_scores_json['Fluency']['Score'])\n",
        "      no_om_score = int(clean_scores_json['No-Omissions']['Score'])\n",
        "      no_ad_score = int(clean_scores_json['No-Additions']['Score'])\n",
        "\n",
        "      if (gram_score not in model_scores) or (flu_score not in model_scores) or (no_om_score not in model_scores) or (no_ad_score not in model_scores):\n",
        "        wrong_score = dp['id']\n",
        "\n",
        "      formatted_scores[\"eid\"] = dp['id']\n",
        "      formatted_scores[\"annotator_id\"] = str(zip_model)\n",
        "      formatted_scores[\"no-omissions\"] = no_om_score\n",
        "      formatted_scores[\"no-additions\"] = no_ad_score\n",
        "      formatted_scores[\"grammaticality\"] = gram_score\n",
        "      formatted_scores[\"fluency\"] = flu_score\n",
        "\n",
        "      print(f\"Gram: {gram_score}; Flu: {flu_score}; NoOm: {no_om_score}; NoAd: {no_ad_score}.\")\n",
        "      print('')\n",
        "\n",
        "  return formatted_scores, eval_missing, wrong_score\n",
        "\n",
        "print(path_dir_unzipped)\n",
        "print(model_prefix)\n",
        "eval_files = glob.glob(os.path.join(path_dir_unzipped, '*'))\n",
        "evals_missing = []\n",
        "wrong_scores = []\n",
        "all_scores = []\n",
        "for filepath in eval_files:\n",
        "  print(filepath)\n",
        "  dbfile_x = open(filepath, 'rb')\n",
        "  formatted_scores, eval_missing, wrong_score = loadDataPoint(dbfile_x, model_prefix)\n",
        "  if eval_missing != None:\n",
        "    evals_missing.append(eval_missing)\n",
        "  if wrong_score != None:\n",
        "    wrong_scores.append(wrong_score)\n",
        "  dbfile_x.close()\n",
        "  all_scores.append(formatted_scores)\n",
        "print(f'Missing evaluations: {evals_missing}')\n",
        "print(f'Wrong scores: {wrong_scores}')\n",
        "\n",
        "# Save all scores into a json file\n",
        "path_json_out = zip_language+'_'+zip_model+'_scores.json'\n",
        "with codecs.open(path_json_out, 'w', 'utf-8') as outfile:\n",
        "  json.dump(all_scores, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(720, 0, 0)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_scores), len(evals_missing), len(wrong_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model         Fluency   Grammaticality   No-Additions No-Omissions\n",
            "agent          7.00           7.00             7.00       6.97\n",
            "e2e            7.00           7.00             7.00       6.95\n",
            "struct         7.00           7.00             7.00       6.36\n",
            "human          6.97           6.96             6.94       6.71\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load your scores file\n",
        "with open('EN_Gemini_scores.json', 'r', encoding='utf-8') as f:\n",
        "    scores = json.load(f)\n",
        "\n",
        "# Mapping: model_name -> list of scores (for each dimension)\n",
        "models = ['agent', 'e2e', 'struct', 'human']\n",
        "dimensions = ['no-omissions', 'no-additions', 'grammaticality', 'fluency']\n",
        "\n",
        "# Set up structure\n",
        "model_scores = {model: {dim: [] for dim in dimensions} for model in models}\n",
        "\n",
        "for entry in scores:\n",
        "    eid = entry.get('eid', '').lower()\n",
        "    for model in models:\n",
        "        if model in eid:  # crude string matching, works with your naming\n",
        "            for dim in dimensions:\n",
        "                val = entry.get(dim)\n",
        "                if isinstance(val, int) or (isinstance(val, str) and val.isdigit()):\n",
        "                    model_scores[model][dim].append(int(val))\n",
        "            break  # Only assign to one model\n",
        "\n",
        "# Now compute means\n",
        "import numpy as np\n",
        "\n",
        "# print(f\"{'Model':<8} {'No-Omissions':>14} {'No-Additions':>14} {'Grammaticality':>16} {'Fluency':>10}\")\n",
        "print(f\"{'Model':<8} {'Fluency':>12} {'Grammaticality':>16} {'No-Additions':>14} {'No-Omissions':>12}\")\n",
        "for model in models:\n",
        "    row = [model]\n",
        "    for dim in dimensions:\n",
        "        vals = model_scores[model][dim]\n",
        "        avg = np.mean(vals) if vals else float('nan')\n",
        "        row.append(f\"{avg:.2f}\")\n",
        "    print(f\"{row[0]:<6} {row[4]:>12} {row[3]:>14} {row[2]:>16} {row[1]:>10}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Gemini:\n",
        "# Model         Fluency   Grammaticality   No-Additions No-Omissions\n",
        "# agent          7.00           7.00             7.00       6.97\n",
        "# e2e            7.00           7.00             7.00       6.95\n",
        "# struct         7.00           7.00             7.00       6.36\n",
        "# human          6.97           6.96             6.94       6.71\n",
        "\n",
        "# # GPT: \n",
        "# Model         Fluency   Grammaticality   No-Additions No-Omissions\n",
        "# agent          6.74           6.97             6.73       6.99\n",
        "# e2e            6.73           6.94             6.78       6.96\n",
        "# struct         6.51           6.87             6.66       5.84\n",
        "# human          6.05           6.27             6.38       6.65"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gemini scores\n",
        "gemini_scores = {\n",
        "    \"agent\":  [7.00, 7.00, 7.00, 6.97],\n",
        "    \"e2e\":    [7.00, 7.00, 7.00, 6.95],\n",
        "    \"struct\": [7.00, 7.00, 7.00, 6.36],\n",
        "    \"human\":  [6.97, 6.96, 6.94, 6.71]\n",
        "}\n",
        "\n",
        "# GPT scores\n",
        "gpt_scores = {\n",
        "    \"agent\":  [6.74, 6.97, 6.73, 6.99],\n",
        "    \"e2e\":    [6.73, 6.94, 6.78, 6.96],\n",
        "    \"struct\": [6.51, 6.87, 6.66, 5.84],\n",
        "    \"human\":  [6.05, 6.27, 6.38, 6.65]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Fluency  Grammaticality  No-Additions  No-Omissions\n",
            "agent     6.870           6.985         6.865         6.980\n",
            "e2e       6.865           6.970         6.890         6.955\n",
            "struct    6.755           6.935         6.830         6.100\n",
            "human     6.510           6.615         6.660         6.680\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "metrics = [\"Fluency\", \"Grammaticality\", \"No-Additions\", \"No-Omissions\"]\n",
        "df_gemini = pd.DataFrame(gemini_scores, index=metrics).T\n",
        "df_gpt = pd.DataFrame(gpt_scores, index=metrics).T\n",
        "\n",
        "# Calculate mean for each model (row-wise), rounding to 2 decimals for clarity\n",
        "df_avg = (df_gemini + df_gpt) / 2\n",
        "df_avg_rounded = df_avg.round(3)\n",
        "print(df_avg_rounded)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lang2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
