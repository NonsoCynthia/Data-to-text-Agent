{"average_bleu": 0.448, "average_meteor": 0.621, "average_rouge_f1": 0.552, "average_comet": 0.808, "average_bertscore_f1": 0.945, "average_bleurt": 0.687}
{"average_bleu": 0.538, "average_meteor": 0.701, "average_rouge_f1": 0.614, "average_comet": 0.837, "average_bertscore_f1": 0.956, "average_bleurt": 0.709}
{"average_bleu": 0.425, "average_meteor": 0.596, "average_rouge_f1": 0.541, "average_comet": 0.812, "average_bertscore_f1": 0.951, "average_bleurt": 0.683}
// Old Prompts 1779 Samples
{"average_bleu": 0.415, "average_meteor": 0.599, "average_rouge_f1": 0.54, "average_comet": 0.81, "average_bertscore_f1": 0.949, "average_bleurt": 0.683}
// First 10 Samples with modified result steps
{"average_bleu": 0.545, "average_meteor": 0.675, "average_rouge_f1": 0.609, "average_comet": 0.833, "average_bertscore_f1": 0.957, "average_bleurt": 0.72}
// First 10 Samples with modified prompt. // The sentence seems very mechanical. Edit the prompt further.
{"average_bleu": 0.537, "average_meteor": 0.74, "average_rouge_f1": 0.646, "average_comet": 0.839, "average_bertscore_f1": 0.955, "average_bleurt": 0.735}
//I further modified the prompt by adding 5 examples to the CO, TS & SR. 10 Samples
{"average_bleu": 0.614, "average_meteor": 0.761, "average_rouge_f1": 0.658, "average_comet": 0.849, "average_bertscore_f1": 0.959, "average_bleurt": 0.733}
// Officially add the instruction field to the Orchestrator. Run 10 Samples
{"average_bleu": 0.624, "average_meteor": 0.788, "average_rouge_f1": 0.704, "average_comet": 0.857, "average_bertscore_f1": 0.96, "average_bleurt": 0.75}
// First 100 samples
{"average_bleu": 0.524, "average_meteor": 0.667, "average_rouge_f1": 0.581, "average_comet": 0.831, "average_bertscore_f1": 0.955, "average_bleurt": 0.711}
// 10 Samples
{"average_bleu": 0.633, "average_meteor": 0.79, "average_rouge_f1": 0.705, "average_comet": 0.857, "average_bertscore_f1": 0.962, "average_bleurt": 0.759}
// Always route guardrail to orchestrator
{"average_bleu": 0.633, "average_meteor": 0.775, "average_rouge_f1": 0.675, "average_comet": 0.856, "average_bertscore_f1": 0.966, "average_bleurt": 0.739}
//Added a condition for the surface realization guardrail to pass to the orchestrator if it finds rerun in the review with Goal SR modification
{"average_bleu": 0.532, "average_meteor": 0.715, "average_rouge_f1": 0.618, "average_comet": 0.837, "average_bertscore_f1": 0.957, "average_bleurt": 0.721}
// Back to old SR prompt but assded As a skilled liguist
{"average_bleu": 0.539, "average_meteor": 0.7, "average_rouge_f1": 0.624, "average_comet": 0.841, "average_bertscore_f1": 0.959, "average_bleurt": 0.737}
// Without skilled linguist
{"average_bleu": 0.543, "average_meteor": 0.717, "average_rouge_f1": 0.632, "average_comet": 0.84, "average_bertscore_f1": 0.96, "average_bleurt": 0.727}
// Worked on the rerouting
{"average_bleu": 0.544, "average_meteor": 0.726, "average_rouge_f1": 0.618, "average_comet": 0.841, "average_bertscore_f1": 0.958, "average_bleurt": 0.718}
//Reroute everything to orchestrator but with old SR Prompt.
{"average_bleu": 0.621, "average_meteor": 0.76, "average_rouge_f1": 0.669, "average_comet": 0.852, "average_bertscore_f1": 0.962, "average_bleurt": 0.743}
// Again
{"average_bleu": 0.6, "average_meteor": 0.778, "average_rouge_f1": 0.662, "average_comet": 0.853, "average_bertscore_f1": 0.961, "average_bleurt": 0.74}
{"average_bleu": 0.579, "average_meteor": 0.765, "average_rouge_f1": 0.667, "average_comet": 0.846, "average_bertscore_f1": 0.96, "average_bleurt": 0.745}
// Monday 07 July
{"average_bleu": 0.588, "average_meteor": 0.764, "average_rouge_f1": 0.681, "average_ter": 0.461, "average_chrf++": 0.769, "average_bleurt": 0.753, "average_bertscore_f1": 0.958, "average_comet": 0.845}
// Change guardrail prompt. 10 Samples
{"average_bleu": 0.64, "average_meteor": 0.781, "average_rouge_f1": 0.706, "average_ter": 0.366, "average_chrf++": 0.791, "average_bleurt": 0.75, "average_bertscore_f1": 0.966, "average_comet": 0.859}
//100 sampples
{"average_bleu": 0.526, "average_meteor": 0.668, "average_rouge_f1": 0.581, "average_ter": 0.51, "average_chrf++": 0.721, "average_bleurt": 0.711, "average_bertscore_f1": 0.957, "average_comet": 0.832}
//modify finalizer -1
{"average_bleu": 0.58, "average_meteor": 0.758, "average_rouge_f1": 0.657, "average_ter": 0.451, "average_chrf++": 0.762, "average_bleurt": 0.735, "average_bertscore_f1": 0.96, "average_comet": 0.848}
{"average_bleu": 0.599, "average_meteor": 0.76, "average_rouge_f1": 0.667, "average_ter": 0.393, "average_chrf++": 0.774, "average_bleurt": 0.735, "average_bertscore_f1": 0.96, "average_comet": 0.849}
// -2 with only workers feedback
{"average_bleu": 0.62, "average_meteor": 0.81, "average_rouge_f1": 0.711, "average_ter": 0.406, "average_chrf++": 0.784, "average_bleurt": 0.754, "average_bertscore_f1": 0.963, "average_comet": 0.859}
{"average_bleu": 0.61, "average_meteor": 0.745, "average_rouge_f1": 0.648, "average_ter": 0.419, "average_chrf++": 0.776, "average_bleurt": 0.729, "average_bertscore_f1": 0.962, "average_comet": 0.853}
{"average_bleu": 0.638, "average_meteor": 0.821, "average_rouge_f1": 0.71, "average_ter": 0.384, "average_chrf++": 0.79, "average_bleurt": 0.741, "average_bertscore_f1": 0.963, "average_comet": 0.859}
// 100 Samples
{"average_bleu": 0.528, "average_meteor": 0.67, "average_rouge_f1": 0.583, "average_ter": 0.516, "average_chrf++": 0.719, "average_bleurt": 0.71, "average_bertscore_f1": 0.956, "average_comet": 0.832}
//1779 samples
{"average_bleu": 0.501, "average_meteor": 0.67, "average_rouge_f1": 0.591, "average_ter": 0.533, "average_chrf++": 0.715, "average_bleurt": 0.718, "average_bertscore_f1": 0.957, "average_comet": 0.831}
