{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9acd5349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: webnlg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Load dataset\n",
    "from agents.dataloader import load_dataset_by_name\n",
    "name = \"webnlg\"\n",
    "data = load_dataset_by_name(name)\n",
    "dataset = list(data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a15efa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Convert your dataset to a list if not already done\n",
    "all_examples = list(dataset)\n",
    "\n",
    "# 1. Count examples per category\n",
    "category_counts = Counter(x['category'] for x in all_examples)\n",
    "\n",
    "# 2. Proportional allocation of 105 samples across categories\n",
    "total_samples = 105\n",
    "category_samples = {\n",
    "    cat: max(1, round(total_samples * count / len(all_examples)))\n",
    "    for cat, count in category_counts.items()\n",
    "}\n",
    "\n",
    "# 3. Group by category\n",
    "by_category = defaultdict(list)\n",
    "for x in all_examples:\n",
    "    by_category[x['category']].append(x)\n",
    "\n",
    "# 4. Randomly sample from each category\n",
    "random.seed(42)\n",
    "sampled = []\n",
    "for cat, items in by_category.items():\n",
    "    n = category_samples[cat]\n",
    "    random.shuffle(items)\n",
    "    sampled.extend(items[:n])\n",
    "\n",
    "# 5. If oversampled, randomly drop extra to get exactly 105\n",
    "if len(sampled) > total_samples:\n",
    "    sampled = random.sample(sampled, total_samples)\n",
    "\n",
    "# 6. Get the indices (IDs) for these 105 samples\n",
    "all_sampled_indices = [item[\"id\"] for item in sampled]\n",
    "\n",
    "# 7. Divide into pilot (first 5) and human evaluation (next 100)\n",
    "pilot_indices = all_sampled_indices[:5]\n",
    "human_eval_indices = all_sampled_indices[5:105]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f0788fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load E2E predictions\n",
    "e2e_preds = {}\n",
    "with open(\"results/webnlg_e2e.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        e2e_preds[entry[\"index\"]] = entry[\"prediction\"]\n",
    "\n",
    "# 2. Load StructGPT4 predictions\n",
    "with open(\"results/factual_struct_gpt_base.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    struct_lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "pilot_rows = []\n",
    "for idx in pilot_indices:\n",
    "    item = dataset[idx]\n",
    "    triples = \"\\n\".join(item.get(\"triples\", item.get(\"input\", [])))\n",
    "    # Reference row\n",
    "    ref = random.choice(item.get(\"references\", []))\n",
    "    pilot_rows.append({\n",
    "        \"index\": item[\"id\"],\n",
    "        \"model\": \"reference\",\n",
    "        \"triples\": triples,\n",
    "        \"reference\": ref,\n",
    "        \"Fluency\": \"\",\n",
    "        \"Grammaticality\": \"\",\n",
    "        \"Addition\": \"\",\n",
    "        \"Omission\": \"\"\n",
    "    })\n",
    "    # E2E row\n",
    "    if item[\"id\"] in e2e_preds:\n",
    "        pilot_rows.append({\n",
    "            \"index\": item[\"id\"],\n",
    "            \"model\": \"E2E\",\n",
    "            \"triples\": triples,\n",
    "            \"reference\": e2e_preds[item[\"id\"]],\n",
    "            \"Fluency\": \"\",\n",
    "            \"Grammaticality\": \"\",\n",
    "            \"Addition\": \"\",\n",
    "            \"Omission\": \"\"\n",
    "        })\n",
    "    # StructGPT4 row (check if line exists)\n",
    "    if item[\"id\"] < len(struct_lines):\n",
    "        pilot_rows.append({\n",
    "            \"index\": item[\"id\"],\n",
    "            \"model\": \"StructGPT4\",\n",
    "            \"triples\": triples,\n",
    "            \"reference\": struct_lines[item[\"id\"]],\n",
    "            \"Fluency\": \"\",\n",
    "            \"Grammaticality\": \"\",\n",
    "            \"Addition\": \"\",\n",
    "            \"Omission\": \"\"\n",
    "        })\n",
    "\n",
    "df_pilot = pd.DataFrame(pilot_rows, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d18294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Evaluation sheet populated with all models and references (AGENT left blank for reference).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "\n",
    "# ---- Load or reuse dataset, human_eval_indices, e2e_preds, struct_lines ----\n",
    "# (Reuse as in previous cells, or define as needed)\n",
    "\n",
    "columns = [\"index\", \"model\", \"triples\", \"reference\", \"Fluency\", \"Grammaticality\", \"Addition\", \"Omission\"]\n",
    "\n",
    "eval_rows = []\n",
    "for idx in human_eval_indices:\n",
    "    item = dataset[idx]\n",
    "    triples = \"\\n\".join(item.get(\"triples\", item.get(\"input\", [])))\n",
    "    # 1. Reference\n",
    "    reference_text = random.choice(item.get(\"references\", []))\n",
    "    eval_rows.append({\n",
    "        \"index\": item[\"id\"],\n",
    "        \"model\": \"reference\",\n",
    "        \"triples\": triples,\n",
    "        \"reference\": reference_text,\n",
    "        \"Fluency\": \"\",\n",
    "        \"Grammaticality\": \"\",\n",
    "        \"Addition\": \"\",\n",
    "        \"Omission\": \"\"\n",
    "    })\n",
    "    # 2. E2E\n",
    "    if item[\"id\"] in e2e_preds:\n",
    "        eval_rows.append({\n",
    "            \"index\": item[\"id\"],\n",
    "            \"model\": \"E2E\",\n",
    "            \"triples\": triples,\n",
    "            \"reference\": e2e_preds[item[\"id\"]],\n",
    "            \"Fluency\": \"\",\n",
    "            \"Grammaticality\": \"\",\n",
    "            \"Addition\": \"\",\n",
    "            \"Omission\": \"\"\n",
    "        })\n",
    "    # 3. StructGPT4\n",
    "    if item[\"id\"] < len(struct_lines):\n",
    "        eval_rows.append({\n",
    "            \"index\": item[\"id\"],\n",
    "            \"model\": \"StructGPT4\",\n",
    "            \"triples\": triples,\n",
    "            \"reference\": struct_lines[item[\"id\"]],\n",
    "            \"Fluency\": \"\",\n",
    "            \"Grammaticality\": \"\",\n",
    "            \"Addition\": \"\",\n",
    "            \"Omission\": \"\"\n",
    "        })\n",
    "    # 4. AGENT (reference blank)\n",
    "    eval_rows.append({\n",
    "        \"index\": item[\"id\"],\n",
    "        \"model\": \"AGENT\",\n",
    "        \"triples\": triples,\n",
    "        \"reference\": \"\",\n",
    "        \"Fluency\": \"\",\n",
    "        \"Grammaticality\": \"\",\n",
    "        \"Addition\": \"\",\n",
    "        \"Omission\": \"\"\n",
    "    })\n",
    "\n",
    "df_eval = pd.DataFrame(eval_rows, columns=columns)\n",
    "\n",
    "# Save or append as a sheet in your Excel\n",
    "with pd.ExcelWriter(\"webnlg_human_eval_study.xlsx\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    df_eval.to_excel(writer, sheet_name=\"Human Evaluation\", index=False)\n",
    "\n",
    "print(\"Human Evaluation sheet populated with all models and references (AGENT left blank for reference).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6239e9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file created: webnlg_human_eval_study.xlsx\n"
     ]
    }
   ],
   "source": [
    "with pd.ExcelWriter(\"webnlg_human_eval_study.xlsx\") as writer:\n",
    "    df_blank.to_excel(writer, sheet_name=\"Sheet1\", index=False)\n",
    "    df_pilot.to_excel(writer, sheet_name=\"Pilot Study\", index=False)\n",
    "    df_eval.to_excel(writer, sheet_name=\"Human Evaluation\", index=False)\n",
    "print(\"Excel file created: webnlg_human_eval_study.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e8adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209afd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59acb38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
